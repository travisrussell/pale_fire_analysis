{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta score for candidate humbert is 1.1486340383396685\n",
      "Delta score for candidate kinbote is 0.9518314112226345\n",
      "Delta score for candidate v is 1.058999372507057\n"
     ]
    }
   ],
   "source": [
    "authors = ['humbert', 'kinbote', 'v', 'canto', 'foreword']\n",
    "potential_authors = ['humbert', 'kinbote', 'v']\n",
    "disputed = 'canto'\n",
    "file_range_map = {\n",
    "    'humbert': range(1,6),\n",
    "    'kinbote': range(1,6),\n",
    "    'v': range(1,6),\n",
    "    'canto': range(1,5),\n",
    "    'foreword': range(1,5)\n",
    "}\n",
    "\n",
    "# A function that compiles all of the text files associated with a single author into a single string\n",
    "def read_files_into_string(filename):\n",
    "    strings = []\n",
    "    for i in file_range_map[filename]:\n",
    "        with open('{}_{}.txt'.format(filename, i)) as f:\n",
    "            strings.append(f.read())\n",
    "    return '\\n'.join(strings)\n",
    "\n",
    "\n",
    "# Make a dictionary out of the authors' corpora\n",
    "work_by_author = {}\n",
    "for author in authors:\n",
    "    work_by_author[author] = read_files_into_string(author)\n",
    "\n",
    "# Load nltk\n",
    "import nltk\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "# Transform the authors' corpora into lists of word tokens\n",
    "work_by_author_tokens = {}\n",
    "work_by_author_length_distributions = {}\n",
    "for author in authors:\n",
    "    tokens = nltk.word_tokenize(work_by_author[author])\n",
    "    work_by_author_tokens[author] = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "\n",
    "\n",
    "# Combine every paper except our test case into a single corpus\n",
    "whole_corpus = []\n",
    "for author in potential_authors:\n",
    "    whole_corpus += work_by_author_tokens[author]\n",
    "    \n",
    "# Get a frequency distribution\n",
    "whole_corpus_freq_dist = list(nltk.FreqDist(whole_corpus).most_common(50))\n",
    "\n",
    "# The main data structure\n",
    "features = [word for word,freq in whole_corpus_freq_dist]\n",
    "feature_freqs = {}\n",
    "\n",
    "for author in potential_authors:\n",
    "    # A dictionary for each candidate's features\n",
    "    feature_freqs[author] = {}\n",
    "    \n",
    "    # A helper value containing the number of tokens in the author's subcorpus\n",
    "    overall = len(work_by_author_tokens[author])\n",
    "    \n",
    "    # Calculate each feature's presence in the subcorpus\n",
    "    for feature in features:\n",
    "        presence = work_by_author_tokens[author].count(feature)\n",
    "        feature_freqs[author][feature] = presence / overall\n",
    "\n",
    "import math\n",
    "\n",
    "# The data structure into which we will be storing the \"corpus standard\" statistics\n",
    "corpus_features = {}\n",
    "\n",
    "# For each feature...\n",
    "for feature in features:\n",
    "    # Create a sub-dictionary that will contain the feature's mean \n",
    "    # and standard deviation\n",
    "    corpus_features[feature] = {}\n",
    "    \n",
    "    # Calculate the mean of the frequencies expressed in the subcorpora\n",
    "    feature_average = 0\n",
    "    for author in potential_authors:\n",
    "        feature_average += feature_freqs[author][feature]\n",
    "    feature_average /= len(authors)\n",
    "    corpus_features[feature][\"Mean\"] = feature_average\n",
    "    \n",
    "    # Calculate the standard deviation using the basic formula for a sample\n",
    "    feature_stdev = 0\n",
    "    for author in potential_authors:\n",
    "        diff = feature_freqs[author][feature] - corpus_features[feature][\"Mean\"]\n",
    "        feature_stdev += diff*diff\n",
    "    feature_stdev /= (len(authors) - 1)\n",
    "    feature_stdev = math.sqrt(feature_stdev)\n",
    "    corpus_features[feature][\"StdDev\"] = feature_stdev\n",
    "\n",
    "    \n",
    "feature_zscores = {}\n",
    "for author in potential_authors:\n",
    "    feature_zscores[author] = {}\n",
    "    for feature in features:\n",
    "        \n",
    "        # Z-score definition = (value - mean) / stddev\n",
    "        # We use intermediate variables to make the code easier to read\n",
    "        feature_val = feature_freqs[author][feature]\n",
    "        feature_mean = corpus_features[feature][\"Mean\"]\n",
    "        feature_stdev = corpus_features[feature][\"StdDev\"]\n",
    "        feature_zscores[author][feature] = ((feature_val-feature_mean) / \n",
    "                                            feature_stdev)\n",
    "\n",
    "# Tokenize the test case\n",
    "testcase_tokens = work_by_author_tokens[disputed]\n",
    " \n",
    "# Calculate the test case's features\n",
    "overall = len(testcase_tokens)\n",
    "testcase_freqs = {}\n",
    "for feature in features:\n",
    "    presence = testcase_tokens.count(feature)\n",
    "    testcase_freqs[feature] = presence / overall\n",
    "    \n",
    "# Calculate the test case's feature z-scores\n",
    "testcase_zscores = {}\n",
    "for feature in features:\n",
    "    feature_val = testcase_freqs[feature]\n",
    "    feature_mean = corpus_features[feature][\"Mean\"]\n",
    "    feature_stdev = corpus_features[feature][\"StdDev\"]\n",
    "    testcase_zscores[feature] = (feature_val - feature_mean) / feature_stdev\n",
    "\n",
    "for author in potential_authors:\n",
    "    delta = 0\n",
    "    for feature in features:\n",
    "        delta += math.fabs((testcase_zscores[feature] - \n",
    "                            feature_zscores[author][feature]))\n",
    "    delta /= len(features)\n",
    "    print( \"Delta score for candidate\", author, \"is\", delta )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}