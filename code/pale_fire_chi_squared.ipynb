{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Chi-squared statistic for candidate humbert is 103.10112417097942\n",
      "The Chi-squared statistic for candidate kinbote is 71.26350902071468\n",
      "The Chi-squared statistic for candidate v is 90.2092520619631\n",
      "The Chi-squared statistic for candidate canto is 143.62251124185974\n",
      "The Chi-squared statistic for candidate foreword is 0.0\n"
     ]
    }
   ],
   "source": [
    "authors = ['humbert', 'kinbote', 'v', 'canto', 'foreword']\n",
    "file_range_map = {\n",
    "    'humbert': range(1,6),\n",
    "    'kinbote': range(1,6),\n",
    "    'v': range(1,6),\n",
    "    'canto': range(1,5),\n",
    "    'foreword': range(1,5)\n",
    "}\n",
    "disputed = 'foreword'\n",
    "\n",
    "# A function that compiles all of the text files associated with a single author into a single string\n",
    "def read_files_into_string(filename):\n",
    "    strings = []\n",
    "    for i in file_range_map[filename]:\n",
    "        with open('{}_{}.txt'.format(filename, i)) as f:\n",
    "            strings.append(f.read())\n",
    "    return '\\n'.join(strings)\n",
    "\n",
    "\n",
    "# Make a dictionary out of the authors' corpora\n",
    "work_by_author = {}\n",
    "for author in authors:\n",
    "    work_by_author[author] = read_files_into_string(author)\n",
    "\n",
    "# Load nltk\n",
    "import nltk\n",
    "matplotlib.use('TkAgg')\n",
    "\n",
    "potential_authors = ['humbert', 'kinbote']\n",
    "\n",
    "# Transform the authors' corpora into lists of word tokens\n",
    "work_by_author_tokens = {}\n",
    "work_by_author_length_distributions = {}\n",
    "for author in authors:\n",
    "    tokens = nltk.word_tokenize(work_by_author[author])\n",
    "\n",
    "    # Filter out punctuation\n",
    "    work_by_author_tokens[author] = ([token for token in tokens if any(c.isalpha() for c in token)])\n",
    "\n",
    "# Lowercase the tokens so that the same word, capitalized or not, \n",
    "# counts as one word\n",
    "for author in authors:\n",
    "    work_by_author_tokens[author] = (\n",
    "        [token.lower() for token in work_by_author_tokens[author]])\n",
    "\n",
    "# Calculate chisquared for each of the two candidate authors\n",
    "for author in authors:\n",
    "   \n",
    "    # First, build a joint corpus and identify the 50 most frequent words in it\n",
    "    joint_corpus = (work_by_author_tokens[author] + \n",
    "                    work_by_author_tokens[disputed])\n",
    "    joint_freq_dist = nltk.FreqDist(joint_corpus)\n",
    "    most_common = list(joint_freq_dist.most_common(50))\n",
    "\n",
    "    # What proportion of the joint corpus is made up \n",
    "    # of the candidate author's tokens?\n",
    "    author_share = (len(work_by_author_tokens[author]) \n",
    "                    / len(joint_corpus))\n",
    "    \n",
    "    # Now, let's look at the 100 most common words in the candidate \n",
    "    # author's corpus and compare the number of times they can be observed \n",
    "    # to what would be expected if the author's papers \n",
    "    # and the Disputed papers were both random samples from the same distribution.\n",
    "    chisquared = 0\n",
    "    for word,joint_count in most_common:\n",
    "        \n",
    "        # How often do we really see this common word?\n",
    "        author_count = work_by_author_tokens[author].count(word)\n",
    "        disputed_count = work_by_author_tokens[disputed].count(word)\n",
    "        \n",
    "        # How often should we see it?\n",
    "        expected_author_count = joint_count * author_share\n",
    "        expected_disputed_count = joint_count * (1-author_share)\n",
    "        \n",
    "        # Add the word's contribution to the chi-squared statistic\n",
    "        chisquared += ((author_count-expected_author_count) * \n",
    "                       (author_count-expected_author_count) / \n",
    "                       expected_author_count)\n",
    "                    \n",
    "        chisquared += ((disputed_count-expected_disputed_count) *\n",
    "                       (disputed_count-expected_disputed_count) \n",
    "                       / expected_disputed_count)\n",
    "        \n",
    "    print(\"The Chi-squared statistic for candidate\", author, \"is\", chisquared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (system-wide)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}